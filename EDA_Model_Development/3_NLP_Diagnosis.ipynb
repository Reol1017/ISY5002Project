{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using NLP to Process Diagnosis Data for Triage Prediction\n",
    "\n",
    "In healthcare, predicting triage and understanding patient influx patterns is essential for effective resource management, particularly in emergency departments.\n",
    "his notebook utilizes natural language processing (NLP) techniques to analyze diagnosis data, aiming to extract insights that can inform predictions about patient severity levels.\n",
    "\n",
    "**Objective**: The primary objective of this notebook is to preprocess and analyze a dataset containing patient diagnoses in order to develop a predictive model for patient severity levels. Specifically, the notebook aims to:\n",
    "- Clean and standardize the diagnosis data.\n",
    "- Apply NLP techniques such as tokenization and lemmatization to prepare the text for analysis.\n",
    "- Create a Bag of Words (BoW) representation for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, essential libraries for natural language processing (NLP) are imported. The spacy library is utilized for advanced NLP tasks such as tokenization and lemmatization. The re library provides regular expression functionalities for text manipulation. The tqdm library is included to offer progress bars during iterative processes, enhancing user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings  # The warnings module to handle warnings during code execution\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English NLP model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial exploration is performed by displaying the the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROWNUM</th>\n",
       "      <th>Hospital</th>\n",
       "      <th>Eligibility Class</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Arrival Time</th>\n",
       "      <th>Severity Level</th>\n",
       "      <th>Deparment</th>\n",
       "      <th>Main Diagnosis</th>\n",
       "      <th>Discharge Time</th>\n",
       "      <th>Waiting Time (Minutes)</th>\n",
       "      <th>Length of Stay (Minutes)</th>\n",
       "      <th>Treatment Time(Minutes)</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>No Treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Royal Commission Health Services Program</td>\n",
       "      <td>ROYAL COMMISSION</td>\n",
       "      <td>Female</td>\n",
       "      <td>2023-12-13 13:17:48</td>\n",
       "      <td>Level Ⅳ</td>\n",
       "      <td>Emergency Medicine</td>\n",
       "      <td>Pain, unspecified</td>\n",
       "      <td>2023-12-13 16:43:00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Royal Commission Health Services Program</td>\n",
       "      <td>ROYAL COMMISSION</td>\n",
       "      <td>Female</td>\n",
       "      <td>2023-12-08 10:59:28</td>\n",
       "      <td>Level Ⅲ</td>\n",
       "      <td>Emergency Medicine</td>\n",
       "      <td>Low back pain</td>\n",
       "      <td>2023-12-08 12:50:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Royal Commission Health Services Program</td>\n",
       "      <td>ROYAL COMMISSION</td>\n",
       "      <td>Female</td>\n",
       "      <td>2023-11-05 14:03:02</td>\n",
       "      <td>Level Ⅲ</td>\n",
       "      <td>Emergency Medicine</td>\n",
       "      <td>Acute upper respiratory infection, unspecified</td>\n",
       "      <td>2023-11-05 14:54:00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Royal Commission Health Services Program</td>\n",
       "      <td>ROYAL COMMISSION</td>\n",
       "      <td>Female</td>\n",
       "      <td>2023-10-07 22:57:41</td>\n",
       "      <td>Level Ⅲ</td>\n",
       "      <td>Emergency Medicine</td>\n",
       "      <td>Epistaxis</td>\n",
       "      <td>2023-10-08 00:09:00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Royal Commission Health Services Program</td>\n",
       "      <td>ROYAL COMMISSION</td>\n",
       "      <td>Female</td>\n",
       "      <td>2023-10-21 21:32:17</td>\n",
       "      <td>Level Ⅳ</td>\n",
       "      <td>Emergency Medicine</td>\n",
       "      <td>Acute upper respiratory infection, unspecified</td>\n",
       "      <td>2023-10-21 23:10:00</td>\n",
       "      <td>56.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ROWNUM                                  Hospital Eligibility Class  Gender  \\\n",
       "0      1  Royal Commission Health Services Program  ROYAL COMMISSION  Female   \n",
       "1      2  Royal Commission Health Services Program  ROYAL COMMISSION  Female   \n",
       "2      3  Royal Commission Health Services Program  ROYAL COMMISSION  Female   \n",
       "3      4  Royal Commission Health Services Program  ROYAL COMMISSION  Female   \n",
       "4      5  Royal Commission Health Services Program  ROYAL COMMISSION  Female   \n",
       "\n",
       "          Arrival Time Severity Level           Deparment  \\\n",
       "0  2023-12-13 13:17:48        Level Ⅳ  Emergency Medicine   \n",
       "1  2023-12-08 10:59:28        Level Ⅲ  Emergency Medicine   \n",
       "2  2023-11-05 14:03:02        Level Ⅲ  Emergency Medicine   \n",
       "3  2023-10-07 22:57:41        Level Ⅲ  Emergency Medicine   \n",
       "4  2023-10-21 21:32:17        Level Ⅳ  Emergency Medicine   \n",
       "\n",
       "                                   Main Diagnosis       Discharge Time  \\\n",
       "0                               Pain, unspecified  2023-12-13 16:43:00   \n",
       "1                                   Low back pain  2023-12-08 12:50:00   \n",
       "2  Acute upper respiratory infection, unspecified  2023-11-05 14:54:00   \n",
       "3                                       Epistaxis  2023-10-08 00:09:00   \n",
       "4  Acute upper respiratory infection, unspecified  2023-10-21 23:10:00   \n",
       "\n",
       "   Waiting Time (Minutes)  Length of Stay (Minutes)  Treatment Time(Minutes)  \\\n",
       "0                    14.0                     205.0                    191.0   \n",
       "1                     7.0                     111.0                    104.0   \n",
       "2                    24.0                      51.0                     27.0   \n",
       "3                    26.0                      71.0                      0.0   \n",
       "4                    56.0                      98.0                     42.0   \n",
       "\n",
       "   Cluster  No Treatment  \n",
       "0        2             0  \n",
       "1        1             0  \n",
       "2        1             0  \n",
       "3        1             1  \n",
       "4        0             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv('../Data/data_after_EDA.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step includes filtering out any records with 'Unrated' severity levels and 'Unknown' main diagnoses, ensuring that the dataset is clean and relevant for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to 'updated_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with 'Unrated' severity and 'Unknown' diagnosis\n",
    "data = data[data['Severity Level'] != 'Unrated']\n",
    "data = data[data['Main Diagnosis'] != 'Unknown']\n",
    "data=data.reset_index()\n",
    "data.drop(columns=['index','ROWNUM'])\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "data.to_csv('../Data/data_after_EDA2.csv', index=False)\n",
    "print(\"Updated dataset saved to 'updated_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new DataFrame is created to focus on the 'Main Diagnosis' column, which is crucial for subsequent NLP tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Main Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pain, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Low back pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acute upper respiratory infection, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Epistaxis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acute upper respiratory infection, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93184</th>\n",
       "      <td>Asthma, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93185</th>\n",
       "      <td>Acute upper respiratory infection, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93186</th>\n",
       "      <td>Acute upper respiratory infection, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93187</th>\n",
       "      <td>Cutaneous abscess, furuncle and carbuncle, uns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93188</th>\n",
       "      <td>Pain in limb, multiple sites</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93189 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Main Diagnosis\n",
       "0                                      Pain, unspecified\n",
       "1                                          Low back pain\n",
       "2         Acute upper respiratory infection, unspecified\n",
       "3                                              Epistaxis\n",
       "4         Acute upper respiratory infection, unspecified\n",
       "...                                                  ...\n",
       "93184                                Asthma, unspecified\n",
       "93185     Acute upper respiratory infection, unspecified\n",
       "93186     Acute upper respiratory infection, unspecified\n",
       "93187  Cutaneous abscess, furuncle and carbuncle, uns...\n",
       "93188                       Pain in limb, multiple sites\n",
       "\n",
       "[93189 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame with only the 'Main Diagnosis' column\n",
    "df = pd.DataFrame(data, columns=['Main Diagnosis'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To standardize the data, all entries in this column are converted to lowercase. \n",
    "- Special characters and punctuation are removed using regular expressions to ensure that the diagnoses are clean and consistent.\n",
    "- Common diagnoses are standardized to reduce redundancy, facilitating better model training later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                         pain unspecified\n",
      "1                                                back pain\n",
      "2                              upper respiratory infection\n",
      "3                                                epistaxis\n",
      "4                              upper respiratory infection\n",
      "                               ...                        \n",
      "93184                                   asthma unspecified\n",
      "93185                          upper respiratory infection\n",
      "93186                          upper respiratory infection\n",
      "93187    cutaneous abscess furuncle and carbuncle unspe...\n",
      "93188                          pain in limb multiple sites\n",
      "Name: Main Diagnosis, Length: 93189, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93189, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Lowercase all the entries in the 'Main Diagnosis' column\n",
    "df['Main Diagnosis'] = df['Main Diagnosis'].str.lower()\n",
    "\n",
    "\n",
    "# Step 2: Remove punctuation or special characters from the diagnoses\n",
    "df['Main Diagnosis'] = df['Main Diagnosis'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Step 3: Standardize some common diagnoses (as an example)\n",
    "df['Main Diagnosis'] = df['Main Diagnosis'].replace({\n",
    "    'acute upper respiratory infection unspecified': 'upper respiratory infection',\n",
    "    'low back pain': 'back pain',\n",
    "   \n",
    "})\n",
    "# Print the cleaned 'Main Diagnosis' column\n",
    "print(df['Main Diagnosis'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "Tokenization is a crucial NLP step that breaks down text into individual words or tokens. A custom function using the spacy library is defined for this purpose. The tokenizer processes the 'Main Diagnosis' column, with a progress bar displayed to monitor the operation. The resulting tokenized data is stored in a new column for easy access in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 93189/93189 [02:06<00:00, 737.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Main Diagnosis  \\\n",
      "0                                       pain unspecified   \n",
      "1                                              back pain   \n",
      "2                            upper respiratory infection   \n",
      "3                                              epistaxis   \n",
      "4                            upper respiratory infection   \n",
      "...                                                  ...   \n",
      "93184                                 asthma unspecified   \n",
      "93185                        upper respiratory infection   \n",
      "93186                        upper respiratory infection   \n",
      "93187  cutaneous abscess furuncle and carbuncle unspe...   \n",
      "93188                        pain in limb multiple sites   \n",
      "\n",
      "                                   Main Diagnosis Tokens  \n",
      "0                                    [pain, unspecified]  \n",
      "1                                           [back, pain]  \n",
      "2                        [upper, respiratory, infection]  \n",
      "3                                            [epistaxis]  \n",
      "4                        [upper, respiratory, infection]  \n",
      "...                                                  ...  \n",
      "93184                              [asthma, unspecified]  \n",
      "93185                    [upper, respiratory, infection]  \n",
      "93186                    [upper, respiratory, infection]  \n",
      "93187  [cutaneous, abscess, furuncle, and, carbuncle,...  \n",
      "93188                  [pain, in, limb, multiple, sites]  \n",
      "\n",
      "[93189 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93189, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization\n",
    "# Function to apply spaCy processing and tokenize the Main Diagnosis column\n",
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Apply spaCy tokenizer to the 'Main Diagnosis' column\n",
    "tqdm.pandas(desc=\"Tokenizing\")\n",
    "df['Main Diagnosis Tokens'] = df['Main Diagnosis'].progress_apply(spacy_tokenizer)\n",
    "\n",
    "# Display the tokenized diagnosis column\n",
    "print(df[['Main Diagnosis', 'Main Diagnosis Tokens']])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stop Word Removal\n",
    "Following tokenization, stop words—common words that add little meaning—are removed. The spacy library's built-in stop words are loaded, and a custom function filters these out from the token lists. This process is also tracked with a progress bar. The updated tokenized data, now free of stop words, is saved in a new column, allowing for a focus on more meaningful terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing Stop Words: 100%|██████████| 93189/93189 [00:00<00:00, 711616.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Main Diagnosis  \\\n",
      "0                                       pain unspecified   \n",
      "1                                              back pain   \n",
      "2                            upper respiratory infection   \n",
      "3                                              epistaxis   \n",
      "4                            upper respiratory infection   \n",
      "...                                                  ...   \n",
      "93184                                 asthma unspecified   \n",
      "93185                        upper respiratory infection   \n",
      "93186                        upper respiratory infection   \n",
      "93187  cutaneous abscess furuncle and carbuncle unspe...   \n",
      "93188                        pain in limb multiple sites   \n",
      "\n",
      "         Main Diagnosis Tokens Without Stopwords  \n",
      "0                                         [pain]  \n",
      "1                                         [pain]  \n",
      "2                [upper, respiratory, infection]  \n",
      "3                                    [epistaxis]  \n",
      "4                [upper, respiratory, infection]  \n",
      "...                                          ...  \n",
      "93184                                   [asthma]  \n",
      "93185            [upper, respiratory, infection]  \n",
      "93186            [upper, respiratory, infection]  \n",
      "93187  [cutaneous, abscess, furuncle, carbuncle]  \n",
      "93188              [pain, limb, multiple, sites]  \n",
      "\n",
      "[93189 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the stop words\n",
    "#  Load spaCy stop words\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stop_words.add(\"unspecified\")\n",
    "\n",
    "# Function to remove stop words from the tokens\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply tqdm to monitor the process\n",
    "tqdm.pandas(desc=\"Removing Stop Words\")\n",
    "\n",
    "# Apply the stop word removal function with a progress bar\n",
    "df['Main Diagnosis Tokens Without Stopwords'] = df['Main Diagnosis Tokens'].progress_apply(remove_stop_words)\n",
    "\n",
    "# Display the updated tokenized diagnosis column without stop words\n",
    "print(df[['Main Diagnosis', 'Main Diagnosis Tokens Without Stopwords']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lemmatization\n",
    "Lemmatization reduces words to their base forms, improving analysis accuracy. A function is created to convert tokens into their lemmas using spacy. This function is applied to the column containing tokens without stop words, with progress monitored via a progress bar. The lemmatized tokens are stored in a new column, providing a simplified yet semantically rich representation of the diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing Tokens: 100%|██████████| 93189/93189 [01:59<00:00, 781.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Main Diagnosis  \\\n",
      "0                                       pain unspecified   \n",
      "1                                              back pain   \n",
      "2                            upper respiratory infection   \n",
      "3                                              epistaxis   \n",
      "4                            upper respiratory infection   \n",
      "...                                                  ...   \n",
      "93184                                 asthma unspecified   \n",
      "93185                        upper respiratory infection   \n",
      "93186                        upper respiratory infection   \n",
      "93187  cutaneous abscess furuncle and carbuncle unspe...   \n",
      "93188                        pain in limb multiple sites   \n",
      "\n",
      "                       Main Diagnosis Lemmatized  \n",
      "0                                         [pain]  \n",
      "1                                         [pain]  \n",
      "2                [upper, respiratory, infection]  \n",
      "3                                    [epistaxis]  \n",
      "4                [upper, respiratory, infection]  \n",
      "...                                          ...  \n",
      "93184                                   [asthma]  \n",
      "93185            [upper, respiratory, infection]  \n",
      "93186            [upper, respiratory, infection]  \n",
      "93187  [cutaneous, abscess, furuncle, carbuncle]  \n",
      "93188               [pain, limb, multiple, site]  \n",
      "\n",
      "[93189 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "# Enable progress bar for pandas\n",
    "tqdm.pandas(desc=\"Lemmatizing Tokens\")\n",
    "\n",
    "# Function to apply spaCy lemmatization to tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))  # Process the tokens into a single string\n",
    "    return [token.lemma_ for token in doc]  # Return the lemmatized tokens\n",
    "\n",
    "# Apply the lemmatization function with a progress bar\n",
    "df['Main Diagnosis Lemmatized'] = df['Main Diagnosis Tokens Without Stopwords'].progress_apply(lemmatize_tokens)\n",
    "\n",
    "# Display the lemmatized tokens\n",
    "print(df[['Main Diagnosis', 'Main Diagnosis Lemmatized']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bag of Words (BoW) Representation\n",
    "To prepare the lemmatized tokens for machine learning, a Bag of Words (BoW) representation is generated. This involves joining the lemmatized tokens back into strings and using CountVectorizer from sklearn to convert the text into numerical vectors. The resulting BoW vectors are stored in a dense array format, and the fitted vectorizer is saved using pickle for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abdominal</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abrasion</th>\n",
       "      <th>abscess</th>\n",
       "      <th>accident</th>\n",
       "      <th>acute</th>\n",
       "      <th>allergy</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>ankle</th>\n",
       "      <th>arthropathy</th>\n",
       "      <th>...</th>\n",
       "      <th>tract</th>\n",
       "      <th>traumatic</th>\n",
       "      <th>upper</th>\n",
       "      <th>urinary</th>\n",
       "      <th>uterine</th>\n",
       "      <th>vaginal</th>\n",
       "      <th>vomit</th>\n",
       "      <th>wheeze</th>\n",
       "      <th>wound</th>\n",
       "      <th>wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93184</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93185</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93186</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93187</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93188</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93189 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abdominal  abnormal  abrasion  abscess  accident  acute  allergy  \\\n",
       "0              0         0         0        0         0      0        0   \n",
       "1              0         0         0        0         0      0        0   \n",
       "2              0         0         0        0         0      0        0   \n",
       "3              0         0         0        0         0      0        0   \n",
       "4              0         0         0        0         0      0        0   \n",
       "...          ...       ...       ...      ...       ...    ...      ...   \n",
       "93184          0         0         0        0         0      0        0   \n",
       "93185          0         0         0        0         0      0        0   \n",
       "93186          0         0         0        0         0      0        0   \n",
       "93187          0         0         0        1         0      0        0   \n",
       "93188          0         0         0        0         0      0        0   \n",
       "\n",
       "       anaemia  ankle  arthropathy  ...  tract  traumatic  upper  urinary  \\\n",
       "0            0      0            0  ...      0          0      0        0   \n",
       "1            0      0            0  ...      0          0      0        0   \n",
       "2            0      0            0  ...      0          0      1        0   \n",
       "3            0      0            0  ...      0          0      0        0   \n",
       "4            0      0            0  ...      0          0      1        0   \n",
       "...        ...    ...          ...  ...    ...        ...    ...      ...   \n",
       "93184        0      0            0  ...      0          0      0        0   \n",
       "93185        0      0            0  ...      0          0      1        0   \n",
       "93186        0      0            0  ...      0          0      1        0   \n",
       "93187        0      0            0  ...      0          0      0        0   \n",
       "93188        0      0            0  ...      0          0      0        0   \n",
       "\n",
       "       uterine  vaginal  vomit  wheeze  wound  wrist  \n",
       "0            0        0      0       0      0      0  \n",
       "1            0        0      0       0      0      0  \n",
       "2            0        0      0       0      0      0  \n",
       "3            0        0      0       0      0      0  \n",
       "4            0        0      0       0      0      0  \n",
       "...        ...      ...    ...     ...    ...    ...  \n",
       "93184        0        0      0       0      0      0  \n",
       "93185        0        0      0       0      0      0  \n",
       "93186        0        0      0       0      0      0  \n",
       "93187        0        0      0       0      0      0  \n",
       "93188        0        0      0       0      0      0  \n",
       "\n",
       "[93189 rows x 110 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create BoW representation\n",
    "# Convert lemmatized tokens back to a single string for vectorization\n",
    "dataset = df['Main Diagnosis Lemmatized'].apply(lambda x: ' '.join(x))\n",
    "vectorizer = CountVectorizer(max_features=110)\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "X_dense = X.toarray()\n",
    "\n",
    "# Save the vectorizer model to a file\n",
    "with open('../Models/nlp_diagnosis.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "    \n",
    "# Creating a DataFrame for the BoW vectors\n",
    "vector_df = pd.DataFrame(X_dense, columns=vectorizer.get_feature_names_out())\n",
    "vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.DataFrame Concatenation\n",
    "The BoW vectors are concatenated with the original DataFrame, creating a comprehensive dataset that includes both original and processed features. Unused intermediate columns are removed to streamline the DataFrame. The final enriched dataset, containing numerical representations of diagnoses, is saved to a new CSV file, preparing it for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Main Diagnosis</th>\n",
       "      <th>Main Diagnosis Tokens</th>\n",
       "      <th>Main Diagnosis Tokens Without Stopwords</th>\n",
       "      <th>Main Diagnosis Lemmatized</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abrasion</th>\n",
       "      <th>abscess</th>\n",
       "      <th>accident</th>\n",
       "      <th>acute</th>\n",
       "      <th>...</th>\n",
       "      <th>tract</th>\n",
       "      <th>traumatic</th>\n",
       "      <th>upper</th>\n",
       "      <th>urinary</th>\n",
       "      <th>uterine</th>\n",
       "      <th>vaginal</th>\n",
       "      <th>vomit</th>\n",
       "      <th>wheeze</th>\n",
       "      <th>wound</th>\n",
       "      <th>wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pain unspecified</td>\n",
       "      <td>[pain, unspecified]</td>\n",
       "      <td>[pain]</td>\n",
       "      <td>[pain]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>back pain</td>\n",
       "      <td>[back, pain]</td>\n",
       "      <td>[pain]</td>\n",
       "      <td>[pain]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>upper respiratory infection</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>epistaxis</td>\n",
       "      <td>[epistaxis]</td>\n",
       "      <td>[epistaxis]</td>\n",
       "      <td>[epistaxis]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>upper respiratory infection</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93184</th>\n",
       "      <td>asthma unspecified</td>\n",
       "      <td>[asthma, unspecified]</td>\n",
       "      <td>[asthma]</td>\n",
       "      <td>[asthma]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93185</th>\n",
       "      <td>upper respiratory infection</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93186</th>\n",
       "      <td>upper respiratory infection</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>[upper, respiratory, infection]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93187</th>\n",
       "      <td>cutaneous abscess furuncle and carbuncle unspe...</td>\n",
       "      <td>[cutaneous, abscess, furuncle, and, carbuncle,...</td>\n",
       "      <td>[cutaneous, abscess, furuncle, carbuncle]</td>\n",
       "      <td>[cutaneous, abscess, furuncle, carbuncle]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93188</th>\n",
       "      <td>pain in limb multiple sites</td>\n",
       "      <td>[pain, in, limb, multiple, sites]</td>\n",
       "      <td>[pain, limb, multiple, sites]</td>\n",
       "      <td>[pain, limb, multiple, site]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93189 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Main Diagnosis  \\\n",
       "0                                       pain unspecified   \n",
       "1                                              back pain   \n",
       "2                            upper respiratory infection   \n",
       "3                                              epistaxis   \n",
       "4                            upper respiratory infection   \n",
       "...                                                  ...   \n",
       "93184                                 asthma unspecified   \n",
       "93185                        upper respiratory infection   \n",
       "93186                        upper respiratory infection   \n",
       "93187  cutaneous abscess furuncle and carbuncle unspe...   \n",
       "93188                        pain in limb multiple sites   \n",
       "\n",
       "                                   Main Diagnosis Tokens  \\\n",
       "0                                    [pain, unspecified]   \n",
       "1                                           [back, pain]   \n",
       "2                        [upper, respiratory, infection]   \n",
       "3                                            [epistaxis]   \n",
       "4                        [upper, respiratory, infection]   \n",
       "...                                                  ...   \n",
       "93184                              [asthma, unspecified]   \n",
       "93185                    [upper, respiratory, infection]   \n",
       "93186                    [upper, respiratory, infection]   \n",
       "93187  [cutaneous, abscess, furuncle, and, carbuncle,...   \n",
       "93188                  [pain, in, limb, multiple, sites]   \n",
       "\n",
       "         Main Diagnosis Tokens Without Stopwords  \\\n",
       "0                                         [pain]   \n",
       "1                                         [pain]   \n",
       "2                [upper, respiratory, infection]   \n",
       "3                                    [epistaxis]   \n",
       "4                [upper, respiratory, infection]   \n",
       "...                                          ...   \n",
       "93184                                   [asthma]   \n",
       "93185            [upper, respiratory, infection]   \n",
       "93186            [upper, respiratory, infection]   \n",
       "93187  [cutaneous, abscess, furuncle, carbuncle]   \n",
       "93188              [pain, limb, multiple, sites]   \n",
       "\n",
       "                       Main Diagnosis Lemmatized  abdominal  abnormal  \\\n",
       "0                                         [pain]          0         0   \n",
       "1                                         [pain]          0         0   \n",
       "2                [upper, respiratory, infection]          0         0   \n",
       "3                                    [epistaxis]          0         0   \n",
       "4                [upper, respiratory, infection]          0         0   \n",
       "...                                          ...        ...       ...   \n",
       "93184                                   [asthma]          0         0   \n",
       "93185            [upper, respiratory, infection]          0         0   \n",
       "93186            [upper, respiratory, infection]          0         0   \n",
       "93187  [cutaneous, abscess, furuncle, carbuncle]          0         0   \n",
       "93188               [pain, limb, multiple, site]          0         0   \n",
       "\n",
       "       abrasion  abscess  accident  acute  ...  tract  traumatic  upper  \\\n",
       "0             0        0         0      0  ...      0          0      0   \n",
       "1             0        0         0      0  ...      0          0      0   \n",
       "2             0        0         0      0  ...      0          0      1   \n",
       "3             0        0         0      0  ...      0          0      0   \n",
       "4             0        0         0      0  ...      0          0      1   \n",
       "...         ...      ...       ...    ...  ...    ...        ...    ...   \n",
       "93184         0        0         0      0  ...      0          0      0   \n",
       "93185         0        0         0      0  ...      0          0      1   \n",
       "93186         0        0         0      0  ...      0          0      1   \n",
       "93187         0        1         0      0  ...      0          0      0   \n",
       "93188         0        0         0      0  ...      0          0      0   \n",
       "\n",
       "       urinary  uterine  vaginal  vomit  wheeze  wound  wrist  \n",
       "0            0        0        0      0       0      0      0  \n",
       "1            0        0        0      0       0      0      0  \n",
       "2            0        0        0      0       0      0      0  \n",
       "3            0        0        0      0       0      0      0  \n",
       "4            0        0        0      0       0      0      0  \n",
       "...        ...      ...      ...    ...     ...    ...    ...  \n",
       "93184        0        0        0      0       0      0      0  \n",
       "93185        0        0        0      0       0      0      0  \n",
       "93186        0        0        0      0       0      0      0  \n",
       "93187        0        0        0      0       0      0      0  \n",
       "93188        0        0        0      0       0      0      0  \n",
       "\n",
       "[93189 rows x 114 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the BoW vectors back to the original DataFrame\n",
    "new_df = pd.concat([df.reset_index(drop=True), vector_df.reset_index(drop=True)], axis=1)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to '../Data/updated_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Remove intermediate columns that are no longer needed\n",
    "new_df=new_df.drop(columns=['Main Diagnosis Tokens','Main Diagnosis Tokens Without Stopwords','Main Diagnosis Lemmatized'])\n",
    "\n",
    "# Saving the updated DataFrame with BoW vectors and Main Diagnosis\n",
    "new_df.to_csv('../Data/updated_dataset.csv', index=False)\n",
    "print(\"Updated dataset saved to '../Data/updated_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Functionality for Model Training\n",
    "In this section, the prepared dataset is used to train a machine learning model with the goal of testing whether the Bag of Words (BoW) vector can accurately predict new inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for the new input: ['Level Ⅳ']\n"
     ]
    }
   ],
   "source": [
    "# Concatenating the dataset of the text CountVector \n",
    "new_df = pd.concat([data, new_df], axis=1)\n",
    "labels = new_df['Severity Level']  \n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting new input\n",
    "new_input = [\"elbow pain\"]\n",
    "new_input_vector = vectorizer.transform([' '.join(spacy_tokenizer(new_input[0]))]).toarray()\n",
    "prediction = model.predict(new_input_vector)\n",
    "\n",
    "print(\"Predicted label for the new input:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Level Ⅰ       0.00      0.00      0.00         3\n",
      "     Level Ⅱ       0.07      0.01      0.02        90\n",
      "     Level Ⅲ       0.56      0.42      0.48      4819\n",
      "     Level Ⅳ       0.77      0.83      0.80     12963\n",
      "     Level Ⅴ       0.16      0.22      0.18       763\n",
      "\n",
      "    accuracy                           0.69     18638\n",
      "   macro avg       0.31      0.30      0.30     18638\n",
      "weighted avg       0.69      0.69      0.69     18638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Assuming you have your predicted labels (y_pred) and actual labels (y_true)\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "In this notebook, we explored the application of natural language processing (NLP) techniques to process diagnosis data and test the functionality of a Bag of Words (BoW) vectorization approach for predicting triage levels in emergency departments. Through steps such as tokenization, stop word removal, and lemmatization, we prepared the data for machine learning. We trained a Naive Bayes classifier to evaluate whether the BoW representation could effectively predict new input diagnoses.\n",
    "While the results obtained demonstrate the potential of this approach, it is important to note that this model serves primarily as a preliminary test of the vectorization technique's capabilities. A dedicated notebook will be developed for more comprehensive model development, focusing on refining the predictive model, optimizing parameters, and enhancing overall performance. This future work will aim to build a robust system that can reliably aid in triage decision-making in real-world healthcare settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Input Tokens: ['sicklecell', 'anaemia', 'crisis']\n",
      "BoW Vector: [[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abdominal</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abrasion</th>\n",
       "      <th>abscess</th>\n",
       "      <th>accident</th>\n",
       "      <th>acute</th>\n",
       "      <th>allergy</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>ankle</th>\n",
       "      <th>arthropathy</th>\n",
       "      <th>...</th>\n",
       "      <th>tract</th>\n",
       "      <th>traumatic</th>\n",
       "      <th>upper</th>\n",
       "      <th>urinary</th>\n",
       "      <th>uterine</th>\n",
       "      <th>vaginal</th>\n",
       "      <th>vomit</th>\n",
       "      <th>wheeze</th>\n",
       "      <th>wound</th>\n",
       "      <th>wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abdominal  abnormal  abrasion  abscess  accident  acute  allergy  anaemia  \\\n",
       "0          0         0         0        0         0      0        0        1   \n",
       "\n",
       "   ankle  arthropathy  ...  tract  traumatic  upper  urinary  uterine  \\\n",
       "0      0            0  ...      0          0      0        0        0   \n",
       "\n",
       "   vaginal  vomit  wheeze  wound  wrist  \n",
       "0        0      0       0      0      0  \n",
       "\n",
       "[1 rows x 110 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the stop words from spaCy\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stop_words.add(\"unspecified\")  # Add any additional stop words if needed\n",
    "\n",
    "# Function for preprocessing new input\n",
    "def preprocess_input(new_input):\n",
    "    # Step 1: Tokenization\n",
    "    tokens = [token.text for token in nlp(new_input)]\n",
    "    \n",
    "    # Step 2: Stop Word Removal\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Step 3: Lemmatization\n",
    "    lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Example of new diagnosis input\n",
    "new_diagnosis = \"sicklecell anaemia with crisis\"\n",
    "\n",
    "# Preprocess the new input\n",
    "processed_input = preprocess_input(new_diagnosis)\n",
    "\n",
    "# Display the processed tokens\n",
    "print(\"Processed Input Tokens:\", processed_input)\n",
    "\n",
    "# If you need to convert to BoW representation for prediction:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "# Load the saved CountVectorizer\n",
    "with open('../Models/nlp_diagnosis.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "\n",
    "# Convert the lemmatized tokens back to a string\n",
    "input_vector = vectorizer.transform([' '.join(processed_input)]).toarray()\n",
    "\n",
    "# Display the BoW vector\n",
    "print(\"BoW Vector:\", input_vector)\n",
    "\n",
    "# Convert the BoW array to a DataFrame\n",
    "vector_df = pd.DataFrame(input_vector, columns=vectorizer.get_feature_names_out())\n",
    "vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
